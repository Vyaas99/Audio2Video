# AudioVisGen: Video Generation Using Audio

## Abstract
This project explores the development of an audio-to-video generation model that integrates Con-
trastive Audio-Visual Pretraining (CAVP) with existing frameworks to enhance video generation from
direct audio inputs. Motivated by the need to improve semantic coherence and temporal synchronization
in generated videos, our approach leverages this integration to address the challenges of translating com-
plex audio cues into visually coherent outputs. While the model showed improvements in aligning audio
with video content, significant discrepancies were observed between quantitative metrics and qualitative
assessments. These discrepancies highlight not only the limitations of current evaluation metrics but also
underscore the inherently subjective nature of art and visual content, which can vary widely in individual
perception. This project points to the necessity for developing more comprehensive evaluation metrics
that better align with human perception and acknowledge the subjective experience of art. The findings
lay the groundwork for future research in enhancing audio-to-video synthesis models, emphasizing the
need for larger datasets, expanded computational resources, and new evaluation methods to truly gauge
the effectiveness and appeal of generated video content.

## Model Structure
<img width="215" alt="Screenshot 2024-05-05 at 18 19 23" src="https://github.com/nidhibali7/rocket/assets/94761618/b5c8c65c-cb34-43a4-a445-25f44ab7f641">

## Example Outputs
![volleyball](https://github.com/nidhibali7/rocket/assets/94761618/78bdf34d-2687-43bb-a5f5-5b584f8af5f5)
![mix_dog_prompt](https://github.com/nidhibali7/rocket/assets/94761618/11b252d8-caf5-4895-b0b0-49b86d52f4bf)
![dog_prompt](https://github.com/nidhibali7/rocket/assets/94761618/31deace9-5179-432a-aab3-e14a7f51189e)
