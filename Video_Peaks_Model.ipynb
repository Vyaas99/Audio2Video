{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install av\n",
        "!pip install torch torchvision torchaudio librosa\n"
      ],
      "metadata": {
        "id": "XRG06oCHw5hW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torchaudio\n",
        "import torchvision.io as io\n",
        "import librosa\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "from torchvision.models import vit_b_16, vit_b_32\n",
        "import av"
      ],
      "metadata": {
        "id": "eXLGFkYhvcpj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(av.__version__)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device:{device}\")\n"
      ],
      "metadata": {
        "id": "FkJNIXtlyIuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7d33kujWvZhF"
      },
      "outputs": [],
      "source": [
        "class AudioVideoDataset(Dataset):\n",
        "    def __init__(self, video_files):\n",
        "        self.video_files = video_files\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_path = self.video_files[idx]\n",
        "        vframes, aframes, info = io.read_video(video_path, pts_unit='sec')\n",
        "        aframes = aframes.mean(0)  #Convert to mono if not done already\n",
        "\n",
        "        # Initialize the transformation\n",
        "        transform = transforms.Compose([\n",
        "            transforms.ToPILImage(),  #Convert the tensors to PIL Images to use Resize\n",
        "            transforms.Resize((224, 224)),  #Resize to 224x224 as expected by the model\n",
        "            transforms.ToTensor()  #Convert back to tensor\n",
        "        ])\n",
        "\n",
        "        # Apply transformation to each frame\n",
        "        vframes_transformed = []\n",
        "        for frame in vframes:\n",
        "            #Ensure the frame is in (C, H, W) format\n",
        "            # print(frame.shape)\n",
        "            frame = frame.permute(2, 0, 1)  #Change from (H, W, C) to (C, H, W)\n",
        "            # print(frame.shape)\n",
        "            frame = transform(frame)  #Apply the transformation\n",
        "            # print(frame.shape)\n",
        "            vframes_transformed.append(frame)\n",
        "        vframes = torch.stack(vframes_transformed)  #Stack the list of frames back into a tensor\n",
        "\n",
        "        onset_times = detect_audio_peaks(video_path)\n",
        "        fps = info['video_fps']\n",
        "        onset_frames = (onset_times * fps).astype(int)\n",
        "        peak_labels = np.zeros(len(vframes))\n",
        "        peak_labels[onset_frames] = 1\n",
        "\n",
        "        peak_labels = torch.tensor(peak_labels).float()\n",
        "        return vframes, peak_labels\n",
        "\n",
        "def detect_audio_peaks(video_path):\n",
        "    y, sr = torchaudio.load(video_path)\n",
        "    y = y.mean(0)  #Convert to mono\n",
        "    onset_env = librosa.onset.onset_strength(y=y.numpy(), sr=sr)\n",
        "    onset_frames = librosa.onset.onset_detect(onset_envelope=onset_env, sr=sr)\n",
        "    onset_times = librosa.frames_to_time(onset_frames, sr=sr)\n",
        "    return onset_times\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def list_video_files(directory, extension=\".mp4\"):\n",
        "    files = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith(extension)]\n",
        "    return files\n",
        "\n",
        "#Path to video folder\n",
        "video_directory = \"/content/drive/MyDrive/Videos\"\n",
        "\n",
        "#Get the list of video file paths\n",
        "video_files = list_video_files(video_directory)\n",
        "\n",
        "#Create the dataset and dataloader\n",
        "dataset = AudioVideoDataset(video_files)\n",
        "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "my5PW9P6vsjI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VideoPeakTransformer(nn.Module):\n",
        "    def __init__(self, num_frames, num_classes=1):\n",
        "        super(VideoPeakTransformer, self).__init__()\n",
        "        #Loading a pre-trained Vision Transformer\n",
        "        self.vit = vit_b_16(pretrained=True)\n",
        "        self.vit.heads = nn.Identity()  #Remove the original classification head\n",
        "\n",
        "        #Time-distributed fully connected layer\n",
        "        self.time_distributed = nn.Linear(768, num_classes) #Directly used 768\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, timesteps, C, H, W = x.shape\n",
        "        x = x.view(batch_size * timesteps, C, H, W)  #Combine batch and timesteps\n",
        "\n",
        "        #Pass each frame through the Vision Transformer\n",
        "        features = self.vit(x)\n",
        "\n",
        "        #Reshape to get back the timesteps dimension\n",
        "        features = features.view(batch_size, timesteps, -1)\n",
        "\n",
        "        #Apply a time-distributed classifier to each timestep\n",
        "        x = self.time_distributed(features)\n",
        "        return torch.sigmoid(x).view(batch_size, timesteps, -1)"
      ],
      "metadata": {
        "id": "XWt-fP36Au9k"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, dataloader, epochs=5):\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for i, (vframes, labels) in enumerate(dataloader):\n",
        "            vframes=vframes.to(device)\n",
        "            labels = labels.unsqueeze(-1)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(vframes)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if i % 10 == 0:\n",
        "                print(f\"Epoch {epoch}, Step {i}, Loss: {loss.item()}\")\n"
      ],
      "metadata": {
        "id": "cOD9r-FJAx0N"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Adjust num_frames to match the typical number of frames you process at once\n",
        "model = VideoPeakTransformer(num_frames=30).to(device)\n",
        "train_model(model, dataloader)\n",
        "torch.save(model.state_dict(), 'video_peak_transformer.pth')\n"
      ],
      "metadata": {
        "id": "Ls6gCjJSA0aQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
